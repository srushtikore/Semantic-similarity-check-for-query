{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEFnnwg6V7Aw",
        "outputId": "2db66402-462e-4bde-80f0-ce243d41611b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.0\n",
            "Collecting distance\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: distance\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16257 sha256=d279491804eeaf1ad8997ddc28508fc67acc0f6a4bdd0815272835c0abe010cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "Successfully built distance\n",
            "Installing collected packages: distance\n",
            "Successfully installed distance-0.1.3\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ],
      "source": [
        "# install Requried Library\n",
        "!pip install nltk\n",
        "!pip install textblob\n",
        "!pip install emoji\n",
        "!pip install distance\n",
        "!pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic library\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# import preprocessing library\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "import emoji\n",
        "from textblob import TextBlob\n",
        "import distance\n",
        "from fuzzywuzzy import fuzz\n",
        "import re\n",
        "\n",
        "import pickle\n",
        "ngram_cv = pickle.load(open('ngram_cv.pkl','rb'))"
      ],
      "metadata": {
        "id": "vhEUNwOpWjOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca49914-8536-4866-c353-9e8968be00c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text PreProcessing\n",
        "def preprocess(quest):\n",
        "  # Lower case\n",
        "  quest=str(quest).lower().strip()\n",
        "\n",
        "  # Remove html tag\n",
        "  quest=BeautifulSoup(quest)\n",
        "  quest=quest.get_text()\n",
        "  #print (quest)\n",
        "  # replae certain special character with  their equivalents\n",
        "\n",
        "  quest = quest.replace('%', ' percent')\n",
        "  quest = quest.replace('$', ' dollar ')\n",
        "  quest = quest.replace('₹', ' rupee ')\n",
        "  quest = quest.replace('€', ' euro ')\n",
        "  quest = quest.replace('@', ' at ')\n",
        "\n",
        "  # The pattern '[math]' appears around 900 times in the whole dataset, we no meaning\n",
        "  quest = quest.replace('[math]','')\n",
        "\n",
        "  # Replacing some number with string equivalents (not perfect, can be done better to account for more cases)\n",
        "\n",
        "  quest = quest.replace(',000,000,000', 'b ')\n",
        "  quest = quest.replace(',000,000 ', 'm ')\n",
        "  quest = quest.replace(',000', 'k')\n",
        "  quest = re.sub(r'([0-9]+)000000000', r'\\1b', quest)\n",
        "  quest = re.sub(r'([0-9]+)00000', r'\\1m', quest)\n",
        "  quest = re.sub(r'([0-9]+)000', r'\\1k',quest)\n",
        "\n",
        "  # chatword treatment\n",
        "\n",
        "  # Decontracting words\n",
        "  # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
        "  # https://stackoverflow.com/a/19794953\n",
        "  contractions = {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"can not\",\n",
        "  \"can't've\": \"can not have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"i'd\": \"i would\",\n",
        "  \"i'd've\": \"i would have\",\n",
        "  \"i'll\": \"i will\",\n",
        "  \"i'll've\": \"i will have\",\n",
        "  \"i'm\": \"i am\",\n",
        "  \"i've\": \"i have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it would\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so as\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there would\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we would\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you would\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you will\",\n",
        "  \"you'll've\": \"you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\",\n",
        "  \"'ve\" : \"have\",\n",
        "  \"n't\" : \"not\",\n",
        "  \"'re\" : \"are\",\n",
        "  \"'ll\" : \"will\"\n",
        "  }\n",
        "\n",
        "  quest_decontracted = []\n",
        "\n",
        "  for word in quest.split():\n",
        "    if word in contractions:\n",
        "\t    word = contractions[word]\n",
        "    quest_decontracted.append(word)\n",
        "\n",
        "  quest = ' '. join(quest_decontracted)\n",
        "\n",
        "  #Remove punctuations\n",
        "  for char in string.punctuation:\n",
        "    quest=quest.replace(char,'')\n",
        "\n",
        "  #Remove url # not needed\n",
        "  #quest = re.compile(r'https?://\\S+|www\\.\\S+').sub('', quest)\n",
        "\n",
        "  # Remove stop word\n",
        "  # will not remove stop word as we will use advance feature engineering\n",
        "  # handling emojis # replace with it's meaning\n",
        "\n",
        "  quest = emoji.demojize(quest)\n",
        "\n",
        "  #spell check . No need\n",
        "  #quest =TextBlob(quest).correct().string\n",
        "  #print(quest)\n",
        "\n",
        "  # Tokenization  # not needed\n",
        "\n",
        "  # Semming   # will use lemmitization\n",
        "\n",
        "  #lemmitization # not needed\n",
        "  #lemmatizer = WordNetLemmatizer()\n",
        "  #quest = ' '.join([lemmatizer.lemmatize(word) for word in quest.split()])\n",
        "\n",
        "  return quest"
      ],
      "metadata": {
        "id": "LuGideU5gROk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction\n",
        "def common_words(q1,q2):\n",
        "\n",
        "  w1 = set(map(lambda word: word.lower().strip(), q1.split(' ')))\n",
        "  w2 = set(map(lambda word: word.lower().strip(), q2.split(' ')))\n",
        "  return len(w1.intersection(w2)) # length of set intersection words, i.e common words betwen tow question\n",
        "\n",
        "def total_words(q1,q2):\n",
        "\n",
        "  w1 = set(map(lambda word: word.lower().strip(), q1.split(' ')))\n",
        "  w2 = set(map(lambda word: word.lower().strip(), q2.split(' ')))\n",
        "  return (len(w1) + len(w2))\n",
        "\n",
        "#Baisc Feature Representation\n",
        "def basic_features(q1,q2):\n",
        "\n",
        "  query = [0.0]*7\n",
        "\n",
        "  query[0]=len(q1)\n",
        "  query[1]=len(q2)\n",
        "  query[2]=len(q1.split(' '))\n",
        "  query[3]=len(q2.split(' '))\n",
        "  query[4]=common_words(q1,q2)\n",
        "  query[5]=total_words(q1,q2)\n",
        "  query[6]=round(common_words(q1,q2)/total_words(q1,q2),2)\n",
        "\n",
        "  return query\n",
        "\n",
        "#Token_advance_features\n",
        "def token_features(q1,q2):\n",
        "\n",
        "  STOP_WORDS = stopwords.words(\"english\")\n",
        "  token_features = [0.0]*8  # as we are returing 8 new featuresm to avoiding the missing of data we will define it in advance\n",
        "\n",
        "  # convert the sentane into tokens:\n",
        "  q1_tokens = q1.split()\n",
        "  q2_tokens = q2.split()\n",
        "\n",
        "  if len(q1_tokens) == 0 or len(q2_tokens) == 0 :\n",
        "    return token_features\n",
        "\n",
        "  # Get the non-stopword in Questions\n",
        "  q1_words =set([word for word in q1_tokens if word not in STOP_WORDS])\n",
        "  q2_words =set([word for word in q2_tokens if word not in STOP_WORDS])\n",
        "\n",
        "  # Get te stopwrods in Question\n",
        "  q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
        "  q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
        "\n",
        "  #Get the common pwords from question pair\n",
        "  common_word_count = len(q1_words.intersection(q2_words))\n",
        "\n",
        "  #get the common stopwords from question pari\n",
        "  common_stop_count = len(q1_stops.intersection(q2_stops))\n",
        "\n",
        "  #get the common Token from Question pair\n",
        "  common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
        "\n",
        "  token_features[0] = common_word_count / (min(len(q1_words),len(q2_words)) + 1) # 1 for safe division\n",
        "  token_features[1] = common_word_count / (max(len(q1_words),len(q2_words)) + 1) # 1 for safe division\n",
        "  token_features[2] = common_stop_count / (min(len(q1_stops),len(q2_stops)) + 1) # 1 for safe division\n",
        "  token_features[3] = common_stop_count / (min(len(q1_stops),len(q2_stops)) + 1) # 1 for safe division\n",
        "  token_features[4] = common_token_count / (min(len(q1_tokens),len(q2_tokens)) + 1) # 1 for safe division\n",
        "  token_features[5] = common_token_count / (max(len(q1_tokens),len(q2_tokens)) + 1) # 1 for safe division\n",
        "  # Lastr word of the question is same or not\n",
        "  token_features[6] = int(q1_tokens[-1]==q2_tokens[-1]) # last word equal\n",
        "  # First word of the question is same or not\n",
        "  token_features[7] = int(q1_tokens[0]==q2_tokens[0]) # first word equal\n",
        "\n",
        "  return token_features\n",
        "\n",
        "#Length_base_advance_features\n",
        "def length_features (q1,q2):\n",
        "\n",
        "  length_features = [0.0]*3\n",
        "\n",
        "  #Sentance to tokens\n",
        "  q1_tokens = q1.split()\n",
        "  q2_tokens = q2.split()\n",
        "\n",
        "  if len(q1_tokens) == 0 or len(q2_tokens) == 0 :\n",
        "    return length_features\n",
        "\n",
        "  # Abs length feature\n",
        "  length_features[0]  = abs(len(q1_tokens) - len(q2_tokens))\n",
        "\n",
        "  #Avg length features\n",
        "  length_features[1] = np.mean(len(q1_tokens) + len(q2_tokens))\n",
        "\n",
        "  strs = list(distance.lcsubstrings(q1, q2))\n",
        "  if strs:\n",
        "    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n",
        "  else:\n",
        "    length_features[2] = 0  # Assign a default value if strs is empty\n",
        "\n",
        "  return length_features\n",
        "\n",
        "#Fuzzybuzzy_advance_features\n",
        "def fuzzy_features(q1,q2):\n",
        "\n",
        "  fuzzy_features = [0.0]*4\n",
        "\n",
        "  #fuzzy ratio\n",
        "  fuzzy_features[0] = fuzz.QRatio(q1,q2)\n",
        "\n",
        "  #fuzzy_partion_ratio\n",
        "  fuzzy_features[1] = fuzz.partial_ratio(q1,q2)\n",
        "\n",
        "  #fuzzy_token_sort_ratio\n",
        "  fuzzy_features[2] = fuzz.token_sort_ratio(q1,q2)\n",
        "\n",
        "  #token_set_ratio\n",
        "  fuzzy_features[3] = fuzz.token_set_ratio(q1,q2)\n",
        "\n",
        "  return fuzzy_features"
      ],
      "metadata": {
        "id": "kcm0J-Q4gzF6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_point_creator(q1,q2):\n",
        "  query = []\n",
        "\n",
        "  #preprocess\n",
        "  q1 = preprocess(q1)\n",
        "  q2 = preprocess(q2)\n",
        "\n",
        "  # fetch basic features\n",
        "  query.extend(basic_features(q1,q2))\n",
        "\n",
        "  #fetch token features\n",
        "  query.extend(token_features(q1,q2))\n",
        "\n",
        "  #fetch length features\n",
        "  query.extend(length_features(q1,q2))\n",
        "\n",
        "  #fetch fuzzy features\n",
        "  query.extend(fuzzy_features(q1,q2))\n",
        "\n",
        "  #w2vfeatures for q1\n",
        "  q1_ngram = ngram_cv.transform([q1]).toarray()\n",
        "\n",
        "  #w2v features for q2\n",
        "  q2_ngram = ngram_cv.transform([q2]).toarray()\n",
        "\n",
        "  return np.hstack((np.array(query).reshape(1, 22), q1_ngram, q2_ngram))"
      ],
      "metadata": {
        "id": "0cc1nMJag1ej"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}